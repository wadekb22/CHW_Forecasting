{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External packages:\n",
    "try:\n",
    "    import cPickle as pickle  # cPickle is faster, but not as complete\n",
    "    \n",
    "except:\n",
    "    import pickle\n",
    "\n",
    "import math\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CHWForecasting:\n",
    "    \n",
    "    def CHWForecasting(self, model_file = None):\n",
    "        \"\"\"Creates a chilled water forcasting object. Loads an earlier model from file if 'model_file' is specified.\"\"\"\n",
    "        self._hourly_model = None\n",
    "        self._daily_model = None\n",
    "        self._hrs_before = None\n",
    "        self._hrs_after = None\n",
    "        self._days_before = None\n",
    "        self._days_after = None\n",
    "        self._daily_significance = None\n",
    "        \n",
    "        if model_file is not None:\n",
    "            # Unpickle here...\n",
    "            self.load(model_file)\n",
    "    \n",
    "    def build_hourly_model(self, hourly_power, hourly_weather, hrs_before, hrs_after):\n",
    "        \"\"\"\n",
    "        Build and train model using 'hourly power' and 'hourly weather' as training data.\n",
    "        \n",
    "        Use hrs_before and hrs_after to specify the range data should be windowed to. hrs_after should match the \n",
    "        numbers of hours the future preditions will be made on.\n",
    "        \"\"\"\n",
    "        hourly = pd.merge(hourly_power, hourly_weather, on = 'DATE')\n",
    "        hourly['DATE'] = hourly.apply(lambda x: pd.Timestamp(x['DATE']), axis=1)\n",
    "        \n",
    "        past_features = hourly.columns.tolist()\n",
    "        past_features.remove('DATE')\n",
    "        \n",
    "        future_features = hourly_weather.columns.tolist()\n",
    "        future_features.remove('DATE')\n",
    "\n",
    "        #past_features = ['Total Power (trimmed)', 'HourlyDryBulbTemperature', 'HourlyWetBulbTemperature']\n",
    "        #future_features = ['HourlyDryBulbTemperature', 'HourlyWetBulbTemperature']\n",
    "\n",
    "        hourlyWindowed = self.windowData(hourly, npast = hrs_before, nfuture = hrs_after, colpast = past_features, colfuture = future_features)\n",
    "        hourlyWindowed = self.genTSfeats(windowedDF = hourlyWindowed, windows = [22])  \n",
    "        hourlyWindowed = self.addTimeFeatures(hourlyWindowed)\n",
    "        X, Ys = self.genXY(hourlyWindowed, 'Total Power (trimmed)', maxoffset = hrs_after)\n",
    "        \n",
    "        all_model = []\n",
    "        all_rmse = []\n",
    "        \n",
    "        for Y in Ys:\n",
    "            model, mae, rmse, rsq = self.runModel(X, Y, model=ExtraTreesRegressor())\n",
    "            all_model.append(model)\n",
    "            all_rmse.append(rmse)\n",
    "        \n",
    "        self._hourly_model = [all_model, all_rmse]\n",
    "        \n",
    "        return self._hourly_model\n",
    "    \n",
    "    def build_daily_model(self, hourly_power, hourly_weather, days_before, days_after, significance = 0.85):\n",
    "        \"\"\"\n",
    "        Build and train model using 'hourly power' and 'hourly weather' as training data.\n",
    "        \n",
    "        Use hrs_before and hrs_after to specify the range data should be windowed to. hrs_after should match the \n",
    "        numbers of hours in the future preditions will be made on.\n",
    "        \"\"\"\n",
    "        train = pd.merge(hourly_power, hourly_weather, on = \"DATE\")\n",
    "        self._daily_significance = significance\n",
    "        self._daily_model = trainGB_ndays(train, signficance = significance)\n",
    "        \n",
    "    def trainGB_ndays(df, predictorVar = 'Total Power (trimmed)_max', dropCols = ['DATE'], significance = 0.85):\n",
    "        gb_modelList = []\n",
    "        for i in range(5):\n",
    "            days = i+1\n",
    "            dailyMaxModels = trainGB_Model(df, days, predictorVar, dropCols, significance) \n",
    "            gb_modelList.append(dailyMaxModels)\n",
    "\n",
    "        return gb_modelList\n",
    "    \n",
    "    def predict_hourly_forecast(self, date_and_time, recent_power, recent_weather, forecasted_weather):\n",
    "        \"\"\" Creates a forecast object for this class using provided data to give predictions. \"\"\"\n",
    "        \n",
    "        recent = pd.merge(recent_power, recent_weather, on = 'DATE')\n",
    "        \n",
    "        past_features = recent.columns.tolist()\n",
    "        past_features.remove('DATE')\n",
    "    \n",
    "        future_features = forecasted_weather.columns.tolist()\n",
    "        future_features.remove('DATE')\n",
    "        \n",
    "        merged = recent.append(forecasted_weather, sort = False)\n",
    "        merged[\"DATE\"] = merged.apply(lambda x: pd.Timestamp(x[\"DATE\"]), axis=1)\n",
    "        merged.reset_index()\n",
    "        \n",
    "        windowed = self.windowData(merged, npast = len(recent_power)-1, nfuture = len(forecasted_weather)-1, colpast = past_features, colfuture = future_features)\n",
    "        windowed_ts = self.genTSfeats(windowedDF = windowed, windows = [22])\n",
    "        self.addTimeFeatures(windowed_ts)\n",
    "        \n",
    "        df = windowed_ts.dropna()\n",
    "        df = df.drop(columns = ['DATE'])\n",
    "        \n",
    "        all_pred = []\n",
    "\n",
    "        for m in self._hourly_model[0]:\n",
    "            all_pred.append(m.predict(df))\n",
    "        \n",
    "        #max_power = max(all_pred) # peak power (tons)\n",
    "        #peak_future_hour = all_pred.index(max_power) \n",
    "        #max_power_error = self._hourly_model[1][peak_future_hour] \n",
    "        #pred_int = (max_power[0] - (2*max_power_error), max_power[0] + (2*max_power_error)) \n",
    "        \n",
    "        #current_date_and_time = pd.to_datetime(date_and_time) # assuming date_and_time gets passed in as a string in the form 'MM-DD-YYYY HH:MM:SS'\n",
    "        #hours_to_add = datetime.timedelta(hours = peak_future_hour + 1)\n",
    "        #peak_timestamp = current_date_and_time + hours_to_add\n",
    "        \n",
    "        #return max_power, peak_timestamp, pred_int\n",
    "        return all_pred\n",
    "    \n",
    "    def predict_daily_forecast(self, recent_power, recent_weather, forecasted_weather):\n",
    "        \"\"\"Creates a forecast object for this class using provided data to give predictions.\"\"\"\n",
    "        predictDf = pd.merge(recent_power, recent_weather.append(forecasted_weather), on = 'DATE', how ='outer')\n",
    "        return predict(predictDF, nDays = len(forecasted_weather.index), self._daily_model)\n",
    "        \n",
    "    def predict(df, nDays, modelList):\n",
    "        test = prepPredict(df, nDays)\n",
    "        date = test['DATE']\n",
    "        testDf = pd.DataFrame(test.values.reshape(1,-1) , columns = test.index)\n",
    "\n",
    "        X = testDf.drop(columns = ['DATE', 'Total Power (trimmed)_max'])\n",
    "\n",
    "        lower = modelList[nDays - 1][0].predict(X)[0]\n",
    "        mid = modelList[nDays - 1][1].predict(X)[0]\n",
    "        upper = modelList[nDays - 1][2].predict(X)[0]\n",
    "\n",
    "        return lower, mid, upper, date\n",
    "    \n",
    "    def windowData(self, df, npast=0, nfuture=0, colpast=[], colfuture=[]):\n",
    "    \n",
    "        # Add past windows:\n",
    "        kept_past = df[colpast]  # elements used in past windows...\n",
    "        for i in range(1, npast+1):\n",
    "            temp = kept_past.shift(i)\n",
    "            temp.columns = [f\"{c}-{i}hr\" for c in kept_past.columns]\n",
    "            df = pd.concat([df,temp], axis=1)\n",
    "        \n",
    "        # Add future windows:\n",
    "        kept_future = df[colfuture]  # elements used future windows...\n",
    "        for i in range(1, nfuture+1):\n",
    "            temp = kept_future.shift(-i)\n",
    "            temp.columns = [f\"{c}+{i}hr\" for c in kept_future.columns]\n",
    "            df = pd.concat([df,temp], axis=1)\n",
    "    \n",
    "        return df\n",
    "    \n",
    "    def genTSfeats(self, windowedDF, windows = []): \n",
    "\n",
    "        for win in windows: # Loop over windows of different sizes, if passed--22 is optimal.\n",
    "\n",
    "            # Generate temporary 'lookback' dataframes for feature construction:\n",
    "            tmp_power = windowedDF.iloc[:, 4:(3*win)+4:3]\n",
    "            tmp_drybulb = windowedDF.iloc[:, 5:(3*win)+5:3]\n",
    "            tmp_wetbulb = windowedDF.iloc[:, 6:(3*win)+6:3]\n",
    "\n",
    "            tmp_dfs = [tmp_power, tmp_drybulb, tmp_wetbulb]\n",
    "            var_names = ['power', 'drybulb', 'wetbulb']\n",
    "\n",
    "            # Loop over temporary dataframes and construct desired features:\n",
    "            for tmp, var in zip(tmp_dfs, var_names):\n",
    "                \n",
    "                # General statistics for base level.\n",
    "                windowedDF[f'meanprev{win}_{var}'] = tmp.mean(axis=1)\n",
    "                windowedDF[f'medianprev{win}_{var}'] = tmp.median(axis=1)\n",
    "                windowedDF[f'minprev{win}_{var}'] = tmp.min(axis=1)\n",
    "                windowedDF[f'maxprev{win}_{var}'] = tmp.max(axis=1)\n",
    "                windowedDF[f'stdprev{win}_{var}'] = tmp.std(axis=1)\n",
    "\n",
    "                # Capturing trend.\n",
    "                windowedDF[f'mean_ewmprev{win}_{var}'] = tmp.T.ewm(com=9.5).mean().T.mean(axis=1)\n",
    "                windowedDF[f'last_ewmprev{win}_{var}'] = tmp.T.ewm(com=9.5).mean().T.iloc[:,-1]\n",
    "                windowedDF[f'avgdiff{win}_{var}'] = (tmp - tmp.shift(1, axis=1)).mean(axis=1)\n",
    "\n",
    "        return windowedDF\n",
    "    \n",
    "    def addTimeFeatures(self, df):\n",
    "        \n",
    "        # Month:\n",
    "        months = df.apply(lambda x: x[\"DATE\"].month, axis=1)\n",
    "        df['month_sin'] = np.sin((months-1)*(2.*np.pi/12))\n",
    "        df['month_cos'] = np.cos((months-1)*(2.*np.pi/12))\n",
    "\n",
    "        # Day of Week:\n",
    "        dow = df.apply(lambda x: x[\"DATE\"].dayofweek, axis=1)\n",
    "        df['DoW_sin'] = np.sin(dow*(2.*np.pi/7))\n",
    "        df['DoW_cos'] = np.cos(dow*(2.*np.pi/7))\n",
    "\n",
    "        # Hour of Day:\n",
    "        hr = df.apply(lambda x: x[\"DATE\"].hour, axis=1)\n",
    "        df['Hour_sin'] = np.sin(hr*(2.*np.pi/24))\n",
    "        df['Hour_cos'] = np.cos(hr*(2.*np.pi/24))\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def genXY(self, X, target_var, maxoffset = 1):\n",
    "        '''\n",
    "        Generate X and Y data to be used in run model. X should be untrimmed (include NaNs).\n",
    "        Will return tuple contianing trimmed X and list of Y vectors with offset ranging from 1 to maxoffset.\n",
    "        '''\n",
    "        df = X.copy()\n",
    "        Y = []\n",
    "\n",
    "        for l in range(1, maxoffset+1):\n",
    "            df[f\"offset_{l}\"] = df[target_var].shift(-l)\n",
    "\n",
    "        df = df.dropna()\n",
    "        df = df.reset_index(drop = True)\n",
    "        df = df.drop(columns = ['DATE'])\n",
    "\n",
    "        for l in range(1, maxoffset+1):\n",
    "            Y.append(df[f\"offset_{l}\"])\n",
    "            df.drop(columns=[f\"offset_{l}\"], inplace = True)\n",
    "\n",
    "        return (df, Y)\n",
    "    \n",
    "    def runModel(self, X, Y, model, verbose=False):\n",
    "\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = 0.8, random_state = 12345, shuffle = True)  # Create train & test sets.\n",
    "        model.fit(X_train, Y_train)  # Fit model...\n",
    "\n",
    "        # Calculate error metrics:\n",
    "        rsq = model.score(X_test, Y_test)\n",
    "        rmse = math.sqrt(metrics.mean_squared_error(Y_test, model.predict(X_test)))\n",
    "        mae = metrics.mean_absolute_error(Y_test, model.predict(X_test))\n",
    "\n",
    "        # Display if selected in arguments:\n",
    "        if verbose:\n",
    "            print(\"R Squared Score: {:.4f}\".format(rsq))\n",
    "            print(\"Root Mean Squared Error: {:.2f}\".format(rmse))\n",
    "            print(\"Mean Absolute Error: {:.2f}\".format(mae))\n",
    "\n",
    "        return model, mae, rmse, rsq\n",
    "    \n",
    "    def formatTrainingData(df, trainDays = 7, nDays = 1):\n",
    "    \n",
    "        df[\"DATE\"] = df.apply(lambda x: pd.Timestamp(x[\"DATE\"]), axis=1)\n",
    "        df['DATE'] = [d.date() for d in df[\"DATE\"]]\n",
    "\n",
    "        groupby = df.groupby('DATE').describe()\n",
    "\n",
    "        colList = df.drop(columns = ['DATE']).columns\n",
    "        parameters = ['min', 'max', 'mean']\n",
    "\n",
    "        dailyData = {'DATE':list(groupby.index)}\n",
    "\n",
    "        nday_cols = []\n",
    "        for i in range(len(colList)):\n",
    "            if(i == 0):\n",
    "                dailyData['Total Power (trimmed)_max'] = list(groupby[colList[i]]['max'])\n",
    "            else:                                     \n",
    "                for param in parameters:\n",
    "                    colname = colList[i] + \"_\" + param\n",
    "                    dailyData[colList[i] + \"_\" + param] = list(groupby[colList[i]][param])\n",
    "                    nday_cols.append(colname)\n",
    "\n",
    "        daily = pd.DataFrame(dailyData)\n",
    "\n",
    "        daily['weekday'] = [row['DATE'].weekday() for i, row in daily.iterrows()]\n",
    "        if(nDays > 0):\n",
    "            for col in nday_cols:\n",
    "                for i in range(nDays + trainDays):\n",
    "                    colname = col + \"-\" + str(i + 1) + \"day\"\n",
    "                    daily[colname] = daily[col].shift(i+1)\n",
    "\n",
    "            for col in ['Total Power (trimmed)_max']:#, 'Total Power (trimmed)_avg']:\n",
    "                for i in range(nDays, trainDays + nDays + 1):\n",
    "                    colname = col + \"-\" + str(i) + \"day\"\n",
    "                    daily[colname] = daily[col].shift(i)\n",
    "\n",
    "        addTimeFeatures(daily)\n",
    "\n",
    "        daily = daily.dropna()\n",
    "        return daily\n",
    "    \n",
    "    def trainGB_Model(df, days, predictorVar, dropCols, significance):\n",
    "        dropCols.append(predictorVar)\n",
    "\n",
    "        daily = formatTrainingData(df, nDays = days)\n",
    "\n",
    "        # Set lower and upper quantile\n",
    "        LOWER_ALPHA = (1 - significance)/2\n",
    "        UPPER_ALPHA = 1-LOWER_ALPHA\n",
    "\n",
    "        # Each model has to be separate\n",
    "        lower = GradientBoostingRegressor(loss=\"quantile\",                   \n",
    "                                                alpha=LOWER_ALPHA)\n",
    "        # The mid model will use the default loss\n",
    "        mid = GradientBoostingRegressor(loss=\"ls\")\n",
    "\n",
    "        upper = GradientBoostingRegressor(loss=\"quantile\",\n",
    "                                                alpha=UPPER_ALPHA)\n",
    "        X = daily.drop(dropCols, axis=1)\n",
    "        Y = daily[predictorVar]\n",
    "\n",
    "        #train the 3 models separately\n",
    "        lower.fit(X, Y)\n",
    "        mid.fit(X, Y)\n",
    "        upper.fit(X, Y)\n",
    "\n",
    "        return [lower, mid, upper]\n",
    "    \n",
    "    def prepPredict(df, nDays, trainDays = 7):\n",
    "\n",
    "        df[\"DATE\"] = df.apply(lambda x: pd.Timestamp(x[\"DATE\"]), axis=1)\n",
    "        df['DATE'] = [d.date() for d in df[\"DATE\"]]\n",
    "\n",
    "        for index in range(len(df)):\n",
    "            if(df.loc[index]['Total Power (trimmed)_max'] != df.loc[index]['Total Power (trimmed)_max']):\n",
    "                today = index-1\n",
    "                predictDay = today + nDays\n",
    "                break\n",
    "        #print(today)\n",
    "\n",
    "        testDf = df.copy()\n",
    "\n",
    "        if(nDays > 0):\n",
    "            for col in testDf.columns[2:]:\n",
    "                for i in range(nDays + trainDays):\n",
    "                    colname = col + \"-\" + str(i + 1) + \"day\"\n",
    "                    testDf[colname] = testDf[col].shift(i+1)\n",
    "\n",
    "            powerCol = testDf.columns[1]\n",
    "            for i in range(nDays, trainDays + nDays + 1):\n",
    "                colname = powerCol + \"-\" + str(i) + \"day\"\n",
    "                testDf[colname] = testDf[powerCol].shift(i)\n",
    "\n",
    "        testDf['weekday'] = [row['DATE'].weekday() for i, row in testDf.iterrows()]\n",
    "        addTimeFeatures(testDf)\n",
    "\n",
    "        return testDf.loc[predictDay]\n",
    "\n",
    "    # load and save are untested, should work though\n",
    "    # https://stackoverflow.com/questions/2709800/how-to-pickle-yourself\n",
    "    \n",
    "    def load(self, filename):\n",
    "        \"\"\"  \"\"\"\n",
    "        f = open(filename, 'rb')\n",
    "        tmp_dict = pickle.load(f)\n",
    "        f.close()          \n",
    "        self.__dict__.clear()\n",
    "        self.__dict__.update(tmp_dict) \n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"  \"\"\"\n",
    "        f = open(filename, 'wb')\n",
    "        pickle.dump(self.__dict__, f, 2)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to ignore Forecast and return max powers and times within predict functions if possible\n",
    "class Forecast:\n",
    "    \n",
    "    def Forecast(self, model_errors, predictions, time):\n",
    "        self._model_errors = model_errors\n",
    "        self._forecasted_power = predictions  # List of future predictions for each future hour\n",
    "        self._init_time = time          # Time prediction was made from\n",
    "    \n",
    "    def get_next_max(self):\n",
    "        \"\"\"Returns time and value of next power maximum within 24 hrs\"\"\"\n",
    "        max_power = max(self._forecasted_power) # peak power (tons)\n",
    "        peak_future_hour = self._forecasted_power.index(max_power) \n",
    "        max_power_error = self._model_errors[peak_future_hour] \n",
    "        pred_int = (max_power[0] - (2*max_power_error), max_power[0] + (2*max_power_error)) \n",
    "        \n",
    "        ## TO DO: Adjust based on how date gets passed in!\n",
    "        current_date_and_time = pd.to_datetime(self._init_time) # assuming date_and_time gets passed in as a string in the form 'MM-DD-YYYY HH:MM:SS'\n",
    "        hours_to_add = datetime.timedelta(hours = peak_future_hour + 1)\n",
    "        peak_timestamp = current_date_and_time + hours_to_add\n",
    "        return max_power, peak_timestamp, pred_int\n",
    "    \n",
    "    def get_daily_max(self):\n",
    "        \"\"\"Returns time and value of next power maximum for each 24 hrs in the forecast\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_power = max(all_pred) # peak power (tons)\n",
    "peak_future_hour = all_pred.index(max_power) \n",
    "max_power_error = self._hourly_model[1][peak_future_hour] \n",
    "pred_int = (max_power[0] - (2*max_power_error), max_power[0] + (2*max_power_error)) \n",
    "        \n",
    "## TO DO: Adjust based on how date gets passed in!\n",
    "current_date_and_time = pd.to_datetime(date_and_time) # assuming date_and_time gets passed in as a string in the form 'MM-DD-YYYY HH:MM:SS'\n",
    "hours_to_add = datetime.timedelta(hours = peak_future_hour + 1)\n",
    "peak_timestamp = current_date_and_time + hours_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>Total Power (trimmed)</th>\n",
       "      <th>HourlyWetBulbTemperature</th>\n",
       "      <th>HourlyDryBulbTemperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-03-01 00:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-03-01 01:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-03-01 02:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-03-01 03:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-03-01 04:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17543</th>\n",
       "      <td>2021-02-28 19:00:00</td>\n",
       "      <td>1365.385205</td>\n",
       "      <td>40.0</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17544</th>\n",
       "      <td>2021-02-28 20:00:00</td>\n",
       "      <td>1396.574994</td>\n",
       "      <td>39.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17545</th>\n",
       "      <td>2021-02-28 21:00:00</td>\n",
       "      <td>1397.580432</td>\n",
       "      <td>39.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17546</th>\n",
       "      <td>2021-02-28 22:00:00</td>\n",
       "      <td>1398.621039</td>\n",
       "      <td>39.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17547</th>\n",
       "      <td>2021-02-28 23:00:00</td>\n",
       "      <td>1392.329980</td>\n",
       "      <td>40.0</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17548 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      DATE  Total Power (trimmed)  HourlyWetBulbTemperature  \\\n",
       "0      2019-03-01 00:00:00               0.000000                      15.0   \n",
       "1      2019-03-01 01:00:00               0.000000                      15.0   \n",
       "2      2019-03-01 02:00:00               0.000000                      15.0   \n",
       "3      2019-03-01 03:00:00               0.000000                      17.0   \n",
       "4      2019-03-01 04:00:00               0.000000                      14.0   \n",
       "...                    ...                    ...                       ...   \n",
       "17543  2021-02-28 19:00:00            1365.385205                      40.0   \n",
       "17544  2021-02-28 20:00:00            1396.574994                      39.0   \n",
       "17545  2021-02-28 21:00:00            1397.580432                      39.0   \n",
       "17546  2021-02-28 22:00:00            1398.621039                      39.0   \n",
       "17547  2021-02-28 23:00:00            1392.329980                      40.0   \n",
       "\n",
       "       HourlyDryBulbTemperature  \n",
       "0                          17.0  \n",
       "1                          17.0  \n",
       "2                          17.0  \n",
       "3                          19.0  \n",
       "4                          16.0  \n",
       "...                         ...  \n",
       "17543                      43.0  \n",
       "17544                      42.0  \n",
       "17545                      42.0  \n",
       "17546                      42.0  \n",
       "17547                      43.0  \n",
       "\n",
       "[17548 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing how to merge the hourly_power and hourly_weather data within build_hourly_model.\n",
    "hourly_power = pd.read_csv('hourly_power.csv', index_col = 0)\n",
    "hourly_weather = pd.read_csv('hourly_weather.csv', index_col = 0)\n",
    "\n",
    "hourly = pd.merge(hourly_power, hourly_weather, on = 'DATE')\n",
    "hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_power = pd.read_csv('recent_power_1.csv', index_col = 0)\n",
    "recent_weather = pd.read_csv('recent_weather_1.csv', index_col = 0)\n",
    "forecasted_weather = pd.read_csv('forecasted_weather_1.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = CHWForecasting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_model_w_errors = forecast.build_hourly_model(hourly_power, hourly_weather, 24, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = forecast.predict_hourly_forecast('2020-04-20 17:00:00', recent_power, recent_weather, forecasted_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_1 = Forecast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_1.Forecast(hourly_model_w_errors[1], predictions, '2020-04-20 17:00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3192.05078125]),\n",
       " Timestamp('2020-04-21 06:00:00'),\n",
       " (2293.3928040236274, 4090.7087584763726))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast_1.get_next_max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([2933.1640625]),\n",
       " array([2924.7671875]),\n",
       " array([2760.359375]),\n",
       " array([2605.6921875]),\n",
       " array([2594.7859375]),\n",
       " array([2592.7390625]),\n",
       " array([2586.4625]),\n",
       " array([2561.6203125]),\n",
       " array([2532.43125]),\n",
       " array([2541.915625]),\n",
       " array([2522.0015625]),\n",
       " array([2658.815625]),\n",
       " array([3192.05078125]),\n",
       " array([3183.471875]),\n",
       " array([2866.303125]),\n",
       " array([2817.9484375]),\n",
       " array([2618.559375]),\n",
       " array([2703.6421875]),\n",
       " array([2852.696875]),\n",
       " array([3027.65]),\n",
       " array([3028.6609375]),\n",
       " array([2768.790625]),\n",
       " array([2720.684375]),\n",
       " array([2686.134375])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Build Model Data/hourly_power.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-e4701215c49e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_power\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Build Model Data/hourly_power.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_weather\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Build Model Data/hourly_weather.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_power\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_weather\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"DATE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Build Model Data/hourly_power.csv'"
     ]
    }
   ],
   "source": [
    "train_power = pd.read_csv('Build Model Data/hourly_power.csv')\n",
    "train_weather = pd.read_csv('Build Model Data/hourly_weather.csv')\n",
    "\n",
    "train = pd.merge(train_power, train_weather, on=\"DATE\")\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
